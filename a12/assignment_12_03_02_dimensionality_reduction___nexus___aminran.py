# -*- coding: utf-8 -*-
"""Assignment 12. 03.02. Dimensionality Reduction | Nexus | aminran.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nb4E7feWT1PuOYYzmVncmjYrYx7a25vU

# PCA on Olivetti Faces Dataset
## Overview
In this assignment, we explore Principal Component Analysis (PCA), a fundamental technique in machine learning for dimensionality reduction and feature extraction. We'll apply PCA to the Olivetti faces dataset to understand how it can be used to compress and reconstruct images.

## Objectives
1. Load and visualize the Olivetti faces dataset.
2. Perform PCA to reduce the dimensionality of the dataset.
3. Determine the optimal number of components using the elbow method.
4. Visualize the effect of PCA on image reconstruction.
5. Compare the performance of a model trained on the original dataset versus the PCA-reduced dataset.

## Prerequisites
Basic understanding of Python and NumPy.
Familiarity with matplotlib for plotting.
Basic knowledge of machine learning concepts, especially PCA.

## Step 1: Load and Visualize the Dataset
"""

# Import necessary libraries
from sklearn.datasets import fetch_olivetti_faces
import numpy as np
import matplotlib.pyplot as plt

# Load the Olivetti faces dataset
faces = fetch_olivetti_faces()
X, y = faces['data'], faces['target']

# Print the shape of the dataset
print(X.shape)

# Select 100 faces randomly for visualization
np.random.seed(0)  # Ensure reproducibility
X_samples = np.random.permutation(X)[:100]

# Plot the selected faces
fig, axes = plt.subplots(10, 10, figsize=(12, 12))
fig.subplots_adjust(hspace=0.01, wspace=0.01)

for i, ax in enumerate(axes.flat):
    ax.imshow(X_samples[i].reshape((64, 64)), cmap='gray')
    ax.set_xticks(())
    ax.set_yticks(())
plt.show()

"""## Step 2: Apply PCA on the Dataset
### Task
* Implement PCA on the dataset.
* Visualize the variance explained by each principal component.
"""

# Import PCA from sklearn
from sklearn.decomposition import PCA

# Apply PCA on the dataset (leave number of components blank)
pca = PCA()
X_pca = pca.fit_transform(X)

# Visualize the variance explained by each principal component
plt.figure(figsize=(8, 4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()

"""## Step 3: Elbow Method for Optimal Components
### Task
* Plot the cumulative explained variance against the number of components.
* Use the elbow method to determine a good number of components for PCA.
"""

# YOUR CODE HERE
# Plot the cumulative explained variance against the number of components
plt.figure(figsize=(8, 4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Elbow Method for Optimal PCA Components')
plt.axhline(y=0.95, color='r', linestyle='--') # Example threshold line at 95% variance
plt.grid(True)
plt.show()

"""## Step 4: Image Reconstruction
### Task
* Reconstruct images using the selected number of PCA components.
* Plot original and reconstructed images side-by-side.
"""

# YOUR CODE HERE for "Reconstruction": use "X_reconstructed"

X_reconstructed = pca.inverse_transform(X_pca)


np.random.seed(0)  # Ensure reproducibility
X_samples = np.random.permutation(X_reconstructed)[:100]

# Plot the selected faces
fig, axes = plt.subplots(10, 10, figsize=(12, 12))
fig.subplots_adjust(hspace=0.01, wspace=0.01)

for i, ax in enumerate(axes.flat):
    ax.imshow(X_samples[i].reshape((64, 64)), cmap='gray')
    ax.set_xticks(())
    ax.set_yticks(())
plt.show()

# Plot original and reconstructed images side-by-side
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
for i in range(5):
    # Original Image
    ax = axes[0, i]
    ax.imshow(X[i].reshape(64, 64), cmap='gray')
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title("Original")

    # Reconstructed Image
    ax = axes[1, i]
    ax.imshow(X_reconstructed[i].reshape(64, 64), cmap='gray')
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title("Reconstructed")

plt.tight_layout()
plt.show()

"""## Step 5: Compare Models
### Task
* Train a model using the original dataset and the PCA-reduced dataset.
* Compare the performance of the two models.
"""

# YOUR CODE HERE
from sklearn.model_selection import train_test_split
# your ML model
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a model on the original dataset
model_original = SVC()
model_original.fit(X_train, y_train)
y_pred_original = model_original.predict(X_test)
accuracy_original = accuracy_score(y_test, y_pred_original)

# Train a model on the PCA-reduced dataset
X_train_pca, X_test_pca = pca.transform(X_train), pca.transform(X_test)
model_pca = SVC()
model_pca.fit(X_train_pca, y_train)
y_pred_pca = model_pca.predict(X_test_pca)
accuracy_pca = accuracy_score(y_test, y_pred_pca)

# Print the accuracy of both models
print(f"Accuracy with original data: {accuracy_original}")
print(f"Accuracy with PCA-reduced data: {accuracy_pca}")

"""## Conclusion
In this assignment, you've learned how to apply PCA on image data, how to choose the number of components, and the effects of PCA on image reconstruction and model performance. This exercise helps in understanding the balance between data reduction and information retention, which is crucial in many machine learning applications.


"""