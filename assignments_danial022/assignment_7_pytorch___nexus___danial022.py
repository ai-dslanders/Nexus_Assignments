# -*- coding: utf-8 -*-
"""Assignment 7. PyTorch | Nexus | danial022.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cx_dPtidzC4hsLNR-HzWDZsJnf1URCcM

# ğŸ”¥ Assignment: Exploring PyTorch

<p align="center">ğŸ“¢âš ï¸ğŸ“‚  </p>

<p align="center"> Please name your file using the format: <code>assignmentName_nickname.py/.ipynb</code> (e.g., <code>project1_ali.py</code>) and push it to GitHub with a clear commit message.</p>

<p align="center"> ğŸš¨ğŸ“ğŸ§  </p>

------------------------------------------------

## ğŸ¯ Goal
This assignment evaluates your basic skills in using PyTorch, focusing on tensors, autograd, and a small training loop.

### 1ï¸âƒ£ Setup and Basics
**Task:**

* Install PyTorch and check its version.

* Create a 2D tensor and print it.
"""

import torch

print("PyTorch version:", torch.__version__)
x = torch.tensor([[1., 2.], [3., 4.]])
print("Tensor x:\n", x)

y = torch.Tensor([[5., 6.],
                 [7., 8.]])
print("Tensor y:\n", y)

"""### 2ï¸âƒ£ Tensor Operations ğŸ§®
**Task:** Perform addition and matrix multiplication.
"""

z = x + y
print("Addition:\n", z)

mat_mul = x @ y
print("Matrix Multiplication:\n", mat_mul)

elem_mul = x * y
print("Element-wise multiplication:\n", elem_mul)

# Your turn:
# Perform element-wise multiplication (x * y) and print the result.

"""### 3ï¸âƒ£ Autograd and Gradients âš™ï¸
**Task:** Enable gradient tracking and compute derivatives.
"""

a = torch.tensor(3.0, requires_grad=True)
b = (a ** 2) + 2 * a + 1
b.backward()
print("Gradient of b wrt a:", a.grad)

p = torch.tensor(2.0, requires_grad=True)
q = p ** 3 + 4 * p
q.backward()
print("Gradient of q wrt p:", p.grad)

# Your turn:
# Create a tensor p with value 2.0 (requires_grad=True) and compute the gradient of q = p^3 + 4p.

"""### 4ï¸âƒ£ Random Tensors ğŸ²
**Task:** Generate a random tensor of shape (2, 3) and find its max and min.
"""

rand_tensor = torch.rand((2, 3))
print("Random Tensor:\n", rand_tensor)
print("Max:", torch.max(rand_tensor))
print("Min:", torch.min(rand_tensor))

"""### 5ï¸âƒ£ Mini Training Loop ğŸ¤–
**Task:** Train a simple linear model y = wx + b using gradient descent.
"""

# Data
x_train = torch.tensor([[1.0], [2.0], [3.0]])
y_train = torch.tensor([[2.0], [4.0], [6.0]])

# Model
w = torch.randn(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# Training
learning_rate = 0.01
for epoch in range(100):
    y_pred = w * x_train + b
    loss = torch.mean((y_pred - y_train) ** 2)
    loss.backward()

    # Update
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
        w.grad.zero_()
        b.grad.zero_()

print("Trained weight:", w.item())
print("Trained bias:", b.item())

"""### 6ï¸âƒ£ Bonus âš¡
* Convert a PyTorch tensor to a NumPy array.

* Convert it back to a PyTorch tensor.
"""

import numpy as np

t = torch.tensor([1., 2., 3.])

np_arr = t.numpy()
print("NumPy array:", np_arr)

t_back = torch.from_numpy(np_arr)
print("Back to tensor:", t_back)